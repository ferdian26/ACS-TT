#!/usr/bin/env python3
"""Galaxy Viewer data calculator

Tool that processes the topic modeling output generated by Mallet and augments it
with other calculated metrics and statistics for use in creating the Galaxy Viewer
visualization.
"""

import argparse
import gzip
import io
import os
import itertools
import numpy as np
import pandas as pd
from time import time
from datetime import timedelta
from handythread import parallel_map, parallel_for
from solr_meta import get_meta

__author__ = "Ryan Chartier, Boris Capitanu"
__version__ = "1.0.0"


class Topic(object):
    def __init__(self, vector, alg=2):
        total = np.sum(vector)
        if total == 1:
            self.value = vector
        elif alg == 1:
            # this is the new algorithm which handles zero better, but it's failing in other areas;
            # kept here for reference and future improvement
            top = 1.2 ** vector
            self.value = top / np.sum(top)
        elif alg == 2:
            # this is the old algorithm which seems to work better in practice
            zeros = vector.size - np.count_nonzero(vector)
            if zeros > 0:
                vector[vector == 0] = float(1) / zeros
                total += 1
            self.value = vector / total
        else:
            raise ValueError("Invalid alg specified ({})".format(alg))

        self._neg = None
        self._length = None

    def __repr__(self):
        return str(self.value)

    def add(self, other):
        top = self.value * other.value
        return Topic(top / np.sum(top))

    def scaler(self, k):
        top = self.value ** k
        return Topic(top / np.sum(top))

    def dot(self, other):
        """Calculates the theoretical dot product
        :param other: The other topic
        :return: The dot product
        """
        powerx = np.log(self.value)
        powery = np.log(other.value)

        total = 0

        for i in range(powerx.size):
            total += np.sum((powerx - powerx[i]) * (powery - powery[i]))

        return total

    @property
    def length(self):
        # This is a variation on the dot product above. It's slightly faster
        # And it normalizes into a length.

        # This is a major problem function!!!!!!
        # Any optimizations on this function are well worth it.

        if self._length is None:
            powerx = np.log(self.value)
            sums = map(lambda x: np.sum((powerx - x) ** 2), powerx)
            self._length = np.sqrt(np.mean(np.fromiter(sums, np.float64, powerx.size)))

        return self._length

    @property
    def neg(self):
        if self._neg is None:
            self._neg = self.scaler(-1)

        return self._neg

    def distance(self, other):
        return self.add(other.neg).length


def read_doc_topics(doc_topics_filename, *, return_num_topics=False):
    """Reads the Mallet-generated file containing the topic allocations per document,
    optionally returning the number of topics found in the file.

    :param doc_topics_filename: The file containing the topic allocations per document
    :param return_num_topics: True if the returned data should include the number of topics found
    :return: (DataFrame[, num_topics]) pair containing the data
    """
    with open(doc_topics_filename, encoding='utf-8') as f:
        data = io.StringIO(f.read())

    with data:
        num_topics = len(data.readline().split()) - 2  # first two columns are 'id' and 'source'
        data.seek(0)  # rewind
        doc_topics = pd.read_csv(data, sep='\t', header=None, engine='c',
                                 names=['id', 'source'] + ['topic.' + str(i) for i in range(num_topics)])

    return doc_topics, num_topics if return_num_topics else doc_topics


def read_topic_keys(topic_keys_filename, *, return_num_topwords=False):
    """Reads the Mallet-generated file containing the top words for each computed topic.

    :param topic_keys_filename: The file containing the top words for each computed topic
    :param return_num_topwords: True if the returned data should include the number of top words found
    :return: (DataFrame[, num_topwords]) pair containing the data
    """
    with open(topic_keys_filename, encoding='utf-8') as f:
        data = io.StringIO(f.read())

    with data:
        num_topwords = len(data.readline().split()) - 2  # first two columns are 'id' and 'alpha'
        data.seek(0)  # rewind
        topic_keys = pd.read_csv(data, sep='\s', header=None, engine='python',
                                 names=['id', 'alpha'] + ['key.' + str(i) for i in range(num_topwords)])

    return topic_keys, num_topwords if return_num_topwords else topic_keys


def read_state(state_filename):
    """Reads the Mallet-generated file containing the topic modeling state (document, token, topic) mapping.

    :param state_filename: The file containing the topic modeling state (document, token, topic) mapping.
    :return: DataFrame containing the state data
    """
    with gzip.open(state_filename) as f:
        state = pd.read_csv(f, sep=' ', header=None, skiprows=3, engine='c', encoding='utf-8',
                            names=['doc', 'source', 'pos', 'typeindex', 'type', 'topic'])
    return state


def prune_state(state, max_dict):
    """Prune the state data by removing all tokens that are not in the top 'max_dict' (by frequency)

    :param state: DataFrame containing the state data
    :param max_dict: A number reflecting the maximum allowed dictionary size.
                     Only the top 'max_dict' tokens (by frequency) will be kept, all others will be pruned.
    :return: The pruned state DataFrame
    """
    top_token_counts = state[['typeindex']].groupby(['typeindex'], sort=False).size().reset_index()
    top_token_counts.columns = ['tokenid', 'count']
    top_token_counts.sort_values(by='count', ascending=False, inplace=True)

    return state[state['typeindex'].isin(top_token_counts.head(max_dict)['tokenid'])]


def ht_id_decode(s):
    """(Overly simplified) helper method for converting a filename representing a HathiTrust volume
    into a proper HathiTrust volume ID.

    :param s: The filename representing a HathiTrust volume (aka the 'clean id')
    :return: The corresponding HathiTrust volume ID
    """
    return s.replace('+', ':').replace('=', '/').replace(',', '.')


def try_parse_number(x):
    """Helper method for identifying and returning numbers, where possible.

    :param x: The potential number
    :return: The number representation of the argument, or the original argument if not a number
    """
    try:
        return int(x)
    except ValueError:
        return x


def read_meta(meta_filename):
    """Reads a CSV metadata file

    :param meta_filename: The CSV metadata filename
    :return: A DataFrame containing the metadata
    :raises ValueError: if the metadata CSV file does not include all required columns
    """
    with open(meta_filename, encoding='utf-8') as f:
        meta = pd.read_csv(f, engine='c')

    for attr in ['source', 'title', 'publishDate']:
        if attr not in meta:
            raise ValueError("Missing metadata attribute: {}".format(attr))

    return meta


def retrieve_meta(doc_topics, solr_url):
    """Retrieves metadata for each document from a SOLR endpoint

    :param doc_topics: The Mallet-generated file containing the topic allocations per document
    :param solr_url: The SOLR endpoint URL
    :return: A DataFrame containing ('id', 'title', 'author', 'publishDate', 'source') for each document;
             if 'publishDate' was missing or invalid, the value '-1' will be assigned to it in the DataFrame
    """
    sources = doc_topics['source'].values
    meta = pd.DataFrame(columns=['id', 'title', 'author', 'publishDate', 'source'], index=np.arange(len(sources)))

    for i, source in enumerate(sources):
        htid = ht_id_decode(os.path.splitext(os.path.split(source)[1])[0])
        try:
            ht_meta = get_meta(htid, {'title', 'author', 'publishDate'}, simplify=True, solr_url=solr_url)
        except:
            ht_meta = []

        if len(ht_meta) > 0:
            ht_meta = ht_meta[0]
            title = ht_meta.get('title')
            if title is not None: title = str(title)
            author = ht_meta.get('author')
            if author is not None: author = str(author)
            publish_date = ht_meta['publishDate']
            if publish_date is None: publish_date = -1
            meta.loc[i] = [htid, title, author, try_parse_number(publish_date), source]
        else:
            print("WARN: Error retrieving (or missing) metadata for {}".format(htid))
            meta.loc[i] = [htid, None, None, -1, source]
            continue

    return meta


def run(doc_topics_filename, topic_keys_filename, state_filename, max_dict, meta_filename, solr_url, output_dir):
    if not os.path.exists(doc_topics_filename):
        raise FileNotFoundError(doc_topics_filename)

    if not os.path.exists(topic_keys_filename):
        raise FileNotFoundError(topic_keys_filename)

    if not os.path.exists(state_filename):
        raise FileNotFoundError(state_filename)

    if meta_filename is not None and not os.path.exists(meta_filename):
        raise FileNotFoundError(meta_filename)

    start = time()

    # read the mallet output
    print("Reading {}...".format(doc_topics_filename), end='', flush=True)
    doc_topics, num_topics = read_doc_topics(doc_topics_filename, return_num_topics=True)
    print("done, {:,} topics, {:,} documents".format(num_topics, len(doc_topics)))

    print("Reading {}...".format(topic_keys_filename), end='', flush=True)
    topic_keys, num_topwords = read_topic_keys(topic_keys_filename, return_num_topwords=True)
    print("done, {:,} top words per topic".format(num_topwords))

    print("Reading {}...".format(state_filename), end='', flush=True)
    state = read_state(state_filename)
    print("done")

    if meta_filename is not None:
        print("Reading metadata from {}...".format(meta_filename), end='', flush=True)
        doc_meta = read_meta(meta_filename)
        print("done, {:,} entries found".format(len(doc_meta)))
    else:
        print("Retrieving metadata from {}...".format(solr_url), end='', flush=True)
        doc_meta = retrieve_meta(doc_topics, solr_url)
        print("done, {:,} entries retrieved".format(len(doc_meta)))

    topicids = range(num_topics)

    # check if pruning is desired
    if max_dict > 0:
        print("Pruning tokens, keeping only the top {:,} tokens by frequency...".format(max_dict), end='', flush=True)
        state = prune_state(state, max_dict)
        print("done")

    print("Processing state data...", end='', flush=True)
    # extract mapping of token id to token
    tokenid_map = state[['typeindex', 'type']].drop_duplicates()
    tokenid_map.columns = ['tokenid', 'token']

    # extract mapping of token -> document -> topic
    state = state[['doc', 'typeindex', 'topic']]
    state.columns = ['docid', 'tokenid', 'topic']

    # compute token, topic -> count mapping
    token_topic_count = state[['tokenid', 'topic']].groupby(['tokenid', 'topic']).size().reset_index()
    token_topic_count.columns = ['tokenid', 'topic', 'count']

    tokenid_max = np.max(token_topic_count['tokenid'])
    print("done, {:,} tokens".format(tokenid_max+1))

    print("Creating topics...", end='', flush=True)

    def create_topic(topicid):
        token_count = token_topic_count[token_topic_count['topic'] == topicid]
        vector = np.repeat(np.float64(0), tokenid_max+1)
        for _, tid, _, cnt in token_count.itertuples():
            vector[tid] = np.float64(cnt)

        return Topic(vector)

    topics = list(parallel_map(create_topic, topicids))
    print("done")

    # Insert dists
    print("Calculating topic distance from center...", end='', flush=True)
    topic_keys['dist'] = list(parallel_map(lambda x: x.length, topics))
    print("done")

    print("Calculating topic means...", end='', flush=True)
    topic_keys['mean'] = doc_topics.ix[:, 'topic.0':].mean().values
    print("done")

    # Create aggregate state object
    print("Calculating aggregate stats...", end='', flush=True)
    agg = state.groupby(['docid', 'tokenid', 'topic']).size().reset_index()
    agg.columns = ['docid', 'tokenid', 'topic', 'count']
    print("done")

    # Create Distance Matrix
    dist = pd.DataFrame(data=0, dtype='float64', index=topicids, columns=topicids)

    print("Calculating inter-topic distances...", end='', flush=True)

    def calc_dist(tuple):
        x, y = tuple
        d = topics[x].distance(topics[y])
        dist[x][y] = d
        dist[y][x] = d

    parallel_for(calc_dist, itertools.combinations(topicids, 2))
    print("done")

    # write results
    print("Writing out results...", end='', flush=True)
    doc_topics.to_csv(os.path.join(output_dir, 'documents.csv'), index=False, encoding='utf-8')
    topic_keys.to_csv(os.path.join(output_dir, 'topics.csv'), index=False, encoding='utf-8')
    tokenid_map.to_csv(os.path.join(output_dir, 'tokens.csv'), index=False, encoding='utf-8')
    agg.to_csv(os.path.join(output_dir, 'state.csv'), index=False, encoding='utf-8')
    dist.to_csv(os.path.join(output_dir, 'distance.csv'), encoding='utf-8')

    if solr_url is not None:
        doc_meta.to_csv(os.path.join(output_dir, 'docmeta.csv'), index=False, encoding='utf-8')
    print("done")

    elapsed = int(time() - start)

    print("All done. Time elapsed: {}".format(timedelta(seconds=elapsed)))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Performs auxiliary computation on Mallet output for use in the Galaxy Viewer",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('--doc-topics', dest='doc_topics_filename', metavar='FILE', default='doctopics.txt',
                        help="The file containing the topic proportions per document")
    parser.add_argument('--topic-keys', dest='topic_keys_filename', metavar='FILE', default='topickeys.txt',
                        help="The file containing the top words for each topic and any Dirichlet parameters")
    parser.add_argument('--state', dest='state_filename', metavar='FILE', default='state.mallet.gz',
                        help="The file containing the Gibbs sampling state produced by Mallet")
    parser.add_argument('--max-dict', dest='max_dict', metavar='N', type=int, default=2000,
                        help="The maximum number of tokens allowed in the dictionary; the top N tokens will be "
                             "used, all others will be pruned. This is needed due for scalability reasons since "
                             "some computations scale with the size of the dictionary. Use N=0 if you want to "
                             "disable the pruning, but expect the running time to increase significantly.")
    parser.add_argument('--output', dest='output_dir', metavar='DIR', required=True,
                        help="The output folder where the results should be written to")

    meta_group = parser.add_mutually_exclusive_group(required=True)
    meta_group.add_argument('--meta', dest='meta_filename',
                            help="The metadata CSV file containing info about each document (at a minimum, the "
                                 "columns must contain 'source', 'title', 'publishDate' - where 'source' must "
                                 "correlate with the 'source' (2nd) column from the '--doc-topics' file)")
    meta_group.add_argument('--solr', dest='solr_url',
                            help="The SOLR base URL to use for looking up document metadata")

    args = parser.parse_args()
    doc_topics_filename = args.doc_topics_filename
    topic_keys_filename = args.topic_keys_filename
    state_filename = args.state_filename
    max_dict = args.max_dict
    output_dir = args.output_dir
    meta_filename = args.meta_filename
    solr_url = args.solr_url

    run(doc_topics_filename, topic_keys_filename, state_filename, max_dict, meta_filename, solr_url, output_dir)
